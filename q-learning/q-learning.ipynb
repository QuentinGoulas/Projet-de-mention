{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we propose to solve the problem on how to play a well known card game known as Black Jack, using reinforcement learning. The rules of the game are as follows.\n",
    "\n",
    "## Card values\n",
    "\n",
    "The game is played with an infinite deck of cards. Each card is associated with a value. The jack, queen and king have a value of 10. The cards between 2 and 10 have a value equal to their number. The ace has a value of either 1 or 11 which is left up to the player. \n",
    "\n",
    "## Start of the game\n",
    "\n",
    "The game is played between a player and a dealer. At the start of the game, the player receives two cards and so does the dealer. If the value of the cards of the player equal 21, then we have a \"natural\", and the player immediately wins, unless the dealer also has a natural, which results in a draw. \n",
    "\n",
    "## Play\n",
    "\n",
    "The dealer shows his first card to the player. It is now the player's turn. the player can request additional cards (a \"hit\") until either he chooses to stop (a \"stand\") or the total value of his cards exceed 21 (he has gone \"bust\"). Once the player is done playing, it is the dealers turn. The dealer plays according to a fixed strategy, where he must \"hit\" until the value of this cards is greater or equal to 17. \n",
    "\n",
    "\n",
    "## Outcome of the game\n",
    "\n",
    "When both the player and the dealer are done, the winner is decided. If the player has gone bust, he loses immediately. If he has not gone bust: he wins if his value is strctly greater than that of the dealer, he loses if his value is strictly smaller than that of the dealer, and otherwise the game is a draw.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Show that the game of Black Jack can be seen as a Markov Decision Process (MDP). Define the state space $\\cal S$, the action space $\\cal A$ and the value of the rewards in each state $r(s,a)$ for $(s,a) \\in {\\cal S} \\times {\\cal A}$.\n",
    "\n",
    "The state space is the player card sum $[12,21]$, the dealer show card $([1,10],ace)$, and wheather or not the player holds a usable ace.\n",
    "The action space is to hit (draw card) or to hit (stop).\n",
    "The reward (-1,0,+1) depending on if the player loses, it's a draw or if the player wins.\n",
    "We don't use discount so those terminal rewards are also the returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 1:** The game can be seen as a MDP where $s = (s_0,s_1,s_2)$ where $s_0$ is the current value of the player, $s_1$ is the value of the card shown by the dealer and $s_2$ indicates whether or not the player holds a usable ace. The action is $a \\in \\{0,1\\}$, where $0$ denotes a stand and $1$ a hit. The rewards are $r(s,a) = 0$ if $s$ is such that the player has not finished playing, and $r(s,a)$ is the expected value obtained when the dealer starts playing in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Complete the \"next_state\" function, which given the current state $s \\in {\\cal S}$ and action $a \\in {\\cal A}$ draws the next state, and indicates whether or not the player has finished playing. For state $s =(14,11,1)$ and action $a = 1$, draw the next state $n=10$ times. Do the same for state $s =(14,11,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_card():\n",
    "    card_value = np.random.choice([2,3,4,5,6,7,8,9,10,10,10,10,11])\n",
    "    return card_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(s,a):\n",
    "    #compute the next state: if the player has hit add value of the drawn card and update usable ace\n",
    "    s_new = list(s)\n",
    "    if a==1:\n",
    "        #draw a card and compute its value\n",
    "        card_value = draw_card()\n",
    "        s_new[0] += card_value\n",
    "        if (card_value == 11): s_new[2] += 1\n",
    "    #if player has a usable ace and he has gone bust then use the ace\n",
    "    if s_new[0] > 21 and s_new[2] >= 1:\n",
    "        s_new[0] -= 10\n",
    "        s_new[2] -= 1\n",
    "    #check if the player has finished playing: either he has not hit or has a value above 21\n",
    "    end_player = True if (a == 0) or (s_new[0] >= 21) else False\n",
    "    #return the new state and whether or not the player is done\n",
    "    return (s_new,end_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(19), 11, 1] False\n",
      "[np.int64(12), 11, 0] False\n",
      "[np.int64(14), 11, 0] False\n",
      "[np.int64(17), 11, 1] False\n",
      "[np.int64(12), 11, 0] False\n",
      "[np.int64(14), 11, 0] False\n",
      "[np.int64(19), 11, 1] False\n",
      "[np.int64(13), 11, 0] False\n",
      "[np.int64(16), 11, 1] False\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    s = [14,11,1]\n",
    "    a = 1\n",
    "    (s_new,end_episode) = next_state(s,a)\n",
    "    print(s_new,end_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Complete the \"get_reward\" function which for any state $s \\in {\\cal S}$ draws the outcome of the game where the dealer starts playing in state $s$. Draw $n=1000$ sample paths to compute the probability that the player wins, draws and loses if the dealer starts playing in state $s = (17,10,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(s):\n",
    "    s_new = list(s)\n",
    "    end_dealer = False\n",
    "    # This considers the case where the dealer has a usable ace\n",
    "    if s_new[1] == 11:\n",
    "        usable_ace_dealer = 1\n",
    "    else:\n",
    "        usable_ace_dealer = 0 \n",
    "    while not end_dealer:\n",
    "        #choose action: the dealer policy always hits until his value is at least 17\n",
    "        if (s_new[1] < 17):\n",
    "            a = 1\n",
    "        else:\n",
    "            a = 0\n",
    "        #draw a card if the dealer has chosen to hit\n",
    "        if a==1:\n",
    "            #draw a card and compute its value\n",
    "            card_value = draw_card()\n",
    "            s_new[1] += card_value\n",
    "            if (card_value == 11): usable_ace_dealer += 1\n",
    "        #if dealer has a usable ace and he has gone bust then use the ace\n",
    "        if s_new[1] > 21 and usable_ace_dealer >= 1: \n",
    "            s_new[1] -= 10\n",
    "            usable_ace_dealer -= 1\n",
    "        #check if the dealer has finished playing\n",
    "        end_dealer = True if s_new[1] >= 17 else False\n",
    "    #output the winner: player loses either if he has gone bust, \n",
    "    #or if he has not gone bust and the dealer has a greater value \n",
    "    #print('Final state',s_new)\n",
    "    if (s_new[0] >= 22):\n",
    "        reward = -1\n",
    "    else:\n",
    "        if (s_new[1] > s_new[0]) and (s_new[1] <= 21):\n",
    "            reward = -1\n",
    "        elif (s_new[1] == s_new[0]):\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: \n",
      "loss 0.7862 draw 0.0 win 0.2137\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "n = 10000\n",
    "for i in range(1,n):\n",
    "    s = [16,10,0]\n",
    "    rewards.append(get_reward(s))\n",
    "print('Probabilities: \\nloss',rewards.count(-1)/n,'draw', rewards.count(0)/n, 'win',rewards.count(1)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Complete the function \"get_episode\" which draws the outcome of a game, where the player uses a threshold strategy with threshold t. Namely, if his value is below or equal to $t$, the player hits, and otherwise he stands. Draw $n=1000$ independent episodes to estimate the expected reward of this policy for $t=16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_episode():\n",
    "    #INITIALIZATION\n",
    "    #initialize the episode \n",
    "    s = [0,0,0]\n",
    "    #give two cards to the player\n",
    "    for i in range(0,2):\n",
    "        card_value = draw_card()\n",
    "        s[0] += card_value\n",
    "        if (card_value == 11): s[2] += 1\n",
    "    #Case where the player has two aces\n",
    "    if s[2] == 2:\n",
    "        s[2] = 1\n",
    "        s[0] -= 10\n",
    "    #give a card to the dealer\n",
    "    s[1] += draw_card()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode(t):\n",
    "    #STEP 1: START\n",
    "    episode_visited_states = []\n",
    "    #deal cards to both players \n",
    "    s = initialize_episode()\n",
    "    #STEP 2: PLAYERS TURN\n",
    "    #player plays according to a fixed policy until he has either gone bust or has chosen to stand\n",
    "    end_player = False\n",
    "    while not end_player:\n",
    "        episode_visited_states.append((s[0],s[1],s[2]))\n",
    "        a = s[0] <= t\n",
    "        (s,end_player) = next_state(s,a)\n",
    "    #STEP 3: DEALERS TURN\n",
    "    #dealer plays by hitting until its value is above 17 and the outcome of the game is decided there\n",
    "    if (a == 1): episode_visited_states.append((s[0],s[1],s[2]))\n",
    "    episode_reward = get_reward(s)\n",
    "    return (episode_reward,episode_visited_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: \n",
      "loss 0.48965 draw 0.10584 win 0.40451\n",
      "Expected reward:  -0.08514 +- 0.006\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "rewards2 = []\n",
    "n = 100000\n",
    "t = 16\n",
    "for i in range(0,n):\n",
    "    (r,h) =  get_episode(t)\n",
    "    rewards.append(r)\n",
    "    rewards2.append(r*r)\n",
    "Er = sum(rewards)/n                            #estimate reward\n",
    "Sr = sum(rewards2)/n - (sum(rewards)/n) ** 2   #estimated reward variance\n",
    "print('Probabilities: \\nloss',rewards.count(-1)/n,'draw', rewards.count(0)/n, 'win',rewards.count(1)/n)\n",
    "print('Expected reward: ',Er,'+-',round(2*math.sqrt(Sr/n),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Repeat the process above for all possible $t$ in order to find the best threshold policy, and comment on this choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold t= 2  expected reward:  -0.182 +- 0.01\n",
      "threshold t= 3  expected reward:  -0.1955 +- 0.01\n",
      "threshold t= 4  expected reward:  -0.1724 +- 0.01\n",
      "threshold t= 5  expected reward:  -0.1787 +- 0.01\n",
      "threshold t= 6  expected reward:  -0.1773 +- 0.01\n",
      "threshold t= 7  expected reward:  -0.1934 +- 0.01\n",
      "threshold t= 8  expected reward:  -0.1809 +- 0.01\n",
      "threshold t= 9  expected reward:  -0.1524 +- 0.01\n",
      "threshold t= 10  expected reward:  -0.1425 +- 0.01\n",
      "threshold t= 11  expected reward:  -0.1111 +- 0.01\n",
      "threshold t= 12  expected reward:  -0.0686 +- 0.01\n",
      "threshold t= 13  expected reward:  -0.0926 +- 0.01\n",
      "threshold t= 14  expected reward:  -0.0805 +- 0.01\n",
      "threshold t= 15  expected reward:  -0.0738 +- 0.009\n",
      "threshold t= 16  expected reward:  -0.071 +- 0.009\n",
      "threshold t= 17  expected reward:  -0.0975 +- 0.009\n",
      "threshold t= 18  expected reward:  -0.2027 +- 0.009\n",
      "threshold t= 19  expected reward:  -0.3407 +- 0.009\n",
      "threshold t= 20  expected reward:  -0.651 +- 0.007\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "for t in range (2,21):\n",
    "    rewards = []\n",
    "    rewards2 = []\n",
    "    for i in range(0,n):\n",
    "        (r,h) =  get_episode(t)\n",
    "        rewards.append(r)\n",
    "        rewards2.append(r*r)\n",
    "    Er = sum(rewards)/n                            #estimate reward\n",
    "    Sr = sum(rewards2)/n - (sum(rewards)/n) ** 2   #estimated reward variance\n",
    "    print('threshold t=',t,' expected reward: ',Er,'+-',round(math.sqrt(Sr/n),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Complete the \"monte_carlo\" function in order to estimate the value function $v^\\pi(s)$ for $\\pi$ the best threshold policy found above, using the Monte Carlo method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(t,I):\n",
    "    #initialize the number of samples and the rewards obseved after passing through each state \n",
    "    R = dict()\n",
    "    N = dict()\n",
    "    V = dict()\n",
    "    for i in range(0,I):\n",
    "        #simulate an episode\n",
    "        (r,h) =  get_episode(t)\n",
    "        for s in h:\n",
    "            #for all states encountered in the episode, update their estimated value\n",
    "            if s in V:\n",
    "                #if s has already appeared in another episode update\n",
    "                R[s] += r\n",
    "                N[s] += 1  \n",
    "            else:\n",
    "                #otherwise add it to the dictionary\n",
    "                R[s] = 0\n",
    "                N[s] = 0\n",
    "            V[s] = R[s] / max([N[s],1])\n",
    "    return V,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No usable ace\n",
      "\n",
      "\n",
      "-0.16\t-0.1\t-0.04\t-0.21\t-0.02\t0.05\t-0.26\t-0.3\t-0.34\t-0.52\t\n",
      "\n",
      "-0.14\t-0.19\t-0.11\t-0.11\t-0.12\t-0.12\t-0.29\t-0.24\t-0.39\t-0.49\t\n",
      "\n",
      "-0.16\t-0.13\t-0.14\t-0.06\t-0.08\t-0.24\t-0.26\t-0.34\t-0.42\t-0.55\t\n",
      "\n",
      "-0.12\t-0.1\t-0.03\t-0.09\t-0.01\t-0.1\t-0.25\t-0.36\t-0.37\t-0.53\t\n",
      "\n",
      "-0.01\t0.01\t-0.0\t-0.02\t0.11\t0.05\t-0.07\t-0.21\t-0.31\t-0.43\t\n",
      "\n",
      "0.09\t0.07\t0.11\t0.09\t0.11\t0.18\t0.09\t-0.05\t-0.21\t-0.3\t\n",
      "\n",
      "0.16\t0.2\t0.2\t0.21\t0.24\t0.27\t0.19\t0.13\t-0.05\t-0.24\t\n",
      "\n",
      "0.19\t0.26\t0.29\t0.24\t0.29\t0.29\t0.2\t0.13\t0.06\t-0.13\t\n",
      "\n",
      "-0.26\t-0.24\t-0.24\t-0.26\t-0.21\t-0.22\t-0.29\t-0.34\t-0.43\t-0.54\t\n",
      "\n",
      "-0.31\t-0.32\t-0.28\t-0.31\t-0.27\t-0.27\t-0.32\t-0.41\t-0.47\t-0.57\t\n",
      "\n",
      "-0.36\t-0.37\t-0.35\t-0.37\t-0.31\t-0.3\t-0.39\t-0.43\t-0.5\t-0.59\t\n",
      "\n",
      "-0.4\t-0.39\t-0.39\t-0.39\t-0.35\t-0.37\t-0.42\t-0.48\t-0.54\t-0.63\t\n",
      "\n",
      "-0.27\t-0.27\t-0.19\t-0.17\t-0.15\t-0.47\t-0.53\t-0.56\t-0.58\t-0.77\t\n",
      "\n",
      "-0.15\t-0.11\t-0.09\t-0.05\t0.02\t-0.11\t-0.39\t-0.43\t-0.46\t-0.64\t\n",
      "\n",
      "0.13\t0.14\t0.2\t0.2\t0.28\t0.4\t0.11\t-0.18\t-0.24\t-0.38\t\n",
      "\n",
      "0.39\t0.41\t0.42\t0.42\t0.48\t0.62\t0.6\t0.28\t-0.01\t-0.1\t\n",
      "\n",
      "0.64\t0.66\t0.66\t0.65\t0.71\t0.78\t0.8\t0.75\t0.44\t0.13\t\n",
      "\n",
      "0.89\t0.89\t0.89\t0.89\t0.91\t0.93\t0.93\t0.93\t0.88\t0.63\t\n",
      " \n",
      "One Usable ace\n",
      "\n",
      "\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "0.07\t0.04\t0.05\t0.15\t0.13\t0.04\t0.02\t-0.11\t-0.21\t-0.31\t\n",
      "\n",
      "-0.04\t-0.03\t0.07\t0.03\t0.01\t0.07\t-0.02\t-0.11\t-0.22\t-0.42\t\n",
      "\n",
      "-0.0\t-0.02\t0.0\t0.06\t0.11\t-0.08\t-0.05\t-0.17\t-0.22\t-0.33\t\n",
      "\n",
      "-0.07\t-0.08\t-0.11\t-0.02\t0.12\t0.0\t-0.07\t-0.13\t-0.26\t-0.44\t\n",
      "\n",
      "-0.28\t-0.34\t-0.24\t-0.18\t-0.14\t-0.51\t-0.58\t-0.48\t-0.61\t-0.73\t\n",
      "\n",
      "-0.14\t-0.09\t-0.08\t0.0\t0.03\t-0.12\t-0.36\t-0.42\t-0.48\t-0.61\t\n",
      "\n",
      "0.15\t0.14\t0.12\t0.18\t0.34\t0.36\t0.17\t-0.16\t-0.24\t-0.45\t\n",
      "\n",
      "0.39\t0.46\t0.38\t0.5\t0.58\t0.58\t0.55\t0.26\t0.0\t-0.09\t\n",
      "\n",
      "0.65\t0.68\t0.63\t0.65\t0.71\t0.79\t0.79\t0.75\t0.46\t0.2\t\n",
      "\n",
      "0.88\t0.88\t0.9\t0.89\t0.9\t0.93\t0.94\t0.94\t0.88\t0.66\t"
     ]
    }
   ],
   "source": [
    "I = 500000\n",
    "V,N = monte_carlo(15,I)\n",
    "print('No usable ace')\n",
    "for i in range(4,22):\n",
    "    print('\\n')\n",
    "    for j in range(2,12):\n",
    "        if (i,j,False) in V:\n",
    "            print(round(V[(i,j,0)],2),end=\"\\t\")\n",
    "        else:\n",
    "            print('?',end=\"\\t\")\n",
    "print('\\n \\nOne Usable ace\\n')\n",
    "for i in range(4,22):\n",
    "    print('\\n')\n",
    "    for j in range(2,12):\n",
    "        if (i,j,True) in V:\n",
    "            print(round(V[(i,j,1)],2),end=\"\\t\")\n",
    "        else:\n",
    "            print('?',end=\"\\t\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.** Visualize the corresponding uncertainties, and find how many sample paths are necessary to get an accurate estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No usable ace\n",
      "\n",
      "\n",
      "0.13\t0.14\t0.13\t0.13\t0.13\t0.13\t0.14\t0.13\t0.07\t0.14\t\n",
      "\n",
      "0.09\t0.09\t0.09\t0.09\t0.09\t0.09\t0.09\t0.09\t0.05\t0.09\t\n",
      "\n",
      "0.07\t0.07\t0.07\t0.07\t0.08\t0.08\t0.08\t0.07\t0.04\t0.07\t\n",
      "\n",
      "0.07\t0.06\t0.06\t0.06\t0.06\t0.06\t0.06\t0.06\t0.03\t0.06\t\n",
      "\n",
      "0.06\t0.06\t0.06\t0.06\t0.06\t0.06\t0.06\t0.06\t0.03\t0.06\t\n",
      "\n",
      "0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.03\t0.05\t\n",
      "\n",
      "0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.02\t0.05\t\n",
      "\n",
      "0.04\t0.04\t0.04\t0.04\t0.04\t0.04\t0.04\t0.04\t0.02\t0.04\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.02\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.02\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.01\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.01\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.02\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.02\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.02\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.02\t0.03\t\n",
      "\n",
      "0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.03\t0.01\t0.03\t\n",
      "\n",
      "0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.05\t0.02\t0.05\t\n",
      " \n",
      "Usable ace\n",
      "\n",
      "\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "0.13\t0.13\t0.14\t0.13\t0.14\t0.14\t0.13\t0.13\t0.07\t0.13\t\n",
      "\n",
      "0.09\t0.09\t0.09\t0.09\t0.09\t0.1\t0.09\t0.09\t0.05\t0.09\t\n",
      "\n",
      "0.09\t0.09\t0.09\t0.09\t0.09\t0.09\t0.09\t0.09\t0.04\t0.09\t\n",
      "\n",
      "0.09\t0.09\t0.09\t0.09\t0.08\t0.08\t0.09\t0.08\t0.04\t0.09\t\n",
      "\n",
      "0.09\t0.08\t0.09\t0.08\t0.08\t0.08\t0.08\t0.08\t0.04\t0.08\t\n",
      "\n",
      "0.08\t0.08\t0.09\t0.08\t0.08\t0.08\t0.08\t0.08\t0.04\t0.08\t\n",
      "\n",
      "0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.04\t0.08\t\n",
      "\n",
      "0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.04\t0.08\t\n",
      "\n",
      "0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.08\t0.04\t0.08\t\n",
      "\n",
      "0.04\t0.04\t0.04\t0.04\t0.04\t0.04\t0.04\t0.04\t0.02\t0.04\t"
     ]
    }
   ],
   "source": [
    "I = 500000\n",
    "V,N = monte_carlo(14,I)\n",
    "print('No usable ace')\n",
    "for i in range(4,22):\n",
    "    print('\\n')\n",
    "    for j in range(2,12):\n",
    "        if (i,j,0) in V:\n",
    "            print(round(2*math.sqrt(1/N[(i,j,0)]),2),end=\"\\t\")\n",
    "        else:\n",
    "            print('?',end=\"\\t\")\n",
    "print('\\n \\nUsable ace\\n')\n",
    "for i in range(4,22):\n",
    "    print('\\n')\n",
    "    for j in range(2,12):\n",
    "        if (i,j,1) in V:\n",
    "            print(round(2*math.sqrt(1/N[(i,j,1)]),2),end=\"\\t\")\n",
    "        else:\n",
    "            print('?',end=\"\\t\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8.** Implement Q-learning in order to estimate the optimal action value function $q^\\star(s,a)$ and the optimal policy $\\pi^\\star$. Visualize the optimal policy and conclude whether or not this policy is a threshold policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(epsilon,discount_factor,learning_rate,I,q0,debug_info):\n",
    "    #initialize the estimated q function \n",
    "    Q = dict()\n",
    "    v = [0,0]\n",
    "    vnew = [0,0]\n",
    "    w = 0\n",
    "    reward_sum = 0\n",
    "    #run Q learning over I epsodes\n",
    "    for i in range(0,I):\n",
    "        if debug_info: print('episode number',i)\n",
    "        #initialize episode\n",
    "        s = initialize_episode()\n",
    "        if debug_info: print('initial state',s)\n",
    "        #STEP 2: PLAYERS TURN\n",
    "        #player plays using the epsilon greedy policy until he has either gone bust or has chosen to stand\n",
    "        end_player = False\n",
    "        while not end_player:\n",
    "            #get Q values for each action in the current state\n",
    "            for i in [0,1]:\n",
    "                v[i] = Q[(s[0],s[1],s[2],i)] if ((s[0],s[1],s[2],i) in Q) else q0\n",
    "            if debug_info: print('action values for current state',v)\n",
    "            #choose best action with probability 1-epsilon\n",
    "            astar = 0 if (v[0] >= v[1]) else 1\n",
    "            a = astar if np.random.randn() > epsilon[i] else np.random.randint(2)\n",
    "            if debug_info: print('chosen action',a)\n",
    "            #observe next state\n",
    "            (snew,end_player) = next_state(s,a)\n",
    "            if debug_info: print('next state',snew)\n",
    "            if debug_info: print('is player done ?',end_player)\n",
    "            #get reward for the current state and action\n",
    "            if not end_player:\n",
    "                reward = 0\n",
    "            else:\n",
    "                if debug_info: print('state before dealers play',snew)\n",
    "                reward = get_reward(snew)\n",
    "            if debug_info: print('reward',reward)\n",
    "            #get maximal Q value for the next state\n",
    "            for i in [0,1]:\n",
    "                if end_player:\n",
    "                    vnew[i] = 0\n",
    "                else:\n",
    "                    vnew[i] = Q[(snew[0],snew[1],snew[2],i)] if ((snew[0],snew[1],snew[2],i) in Q) else q0\n",
    "            if debug_info: print('next state action values',vnew)\n",
    "            #update q values \n",
    "            if (s[0],s[1],s[2],a) in Q:\n",
    "                Q[(s[0],s[1],s[2],a)] += learning_rate*(reward + discount_factor*max(vnew) - v[a])\n",
    "            else:\n",
    "                Q[(s[0],s[1],s[2],a)] = q0 + learning_rate*(reward + discount_factor*max(vnew) - v[a])\n",
    "            #update state\n",
    "            s = snew\n",
    "        reward_sum += reward\n",
    "        if debug_info: print('episode reward',reward)\n",
    "        if debug_info: print('Qtable',Q)\n",
    "    return Q, reward_sum/I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No usable ace\n",
      "\n",
      "\n",
      "hit\thit\thit\tstand\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\tstand\tstand\thit\tstand\thit\thit\thit\thit\thit\t\n",
      "\n",
      "stand\thit\thit\tstand\tstand\thit\tstand\thit\thit\thit\t\n",
      "\n",
      "hit\tstand\thit\tstand\tstand\thit\thit\thit\thit\thit\t\n",
      "\n",
      "stand\tstand\tstand\thit\tstand\thit\thit\thit\thit\thit\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\thit\thit\tstand\thit\tstand\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\thit\tstand\thit\tstand\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      " \n",
      "One Usable ace\n",
      "\n",
      "\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "?\t?\t?\t?\t?\t?\t?\t?\t?\t?\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "hit\thit\thit\thit\thit\thit\thit\thit\thit\thit\t\n",
      "\n",
      "stand\thit\tstand\tstand\thit\tstand\tstand\thit\tstand\thit\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\t\n",
      "\n",
      "stand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\tstand\t"
     ]
    }
   ],
   "source": [
    "epsilons = 1 / (1 + np.arange(I))\n",
    "Q,_ = qlearning(epsilons,1,0.03,1000000,0,False)\n",
    "\n",
    "print('No usable ace')\n",
    "for i in range(4,22):\n",
    "    print('\\n')\n",
    "    for j in range(2,12):\n",
    "        if (i,j,0,0) in Q and (i,j,0,1) in Q:\n",
    "            if Q[(i,j,0,0)] < Q[(i,j,0,1)]:\n",
    "                print('hit',end=\"\\t\")\n",
    "            else:\n",
    "                print('stand',end=\"\\t\")\n",
    "        else:\n",
    "            print('?',end=\"\\t\")\n",
    "print('\\n \\nOne Usable ace\\n')\n",
    "for i in range(4,22):\n",
    "    print('\\n')\n",
    "    for j in range(2,12):\n",
    "        if (i,j,1,0) in Q and (i,j,1,1) in Q:\n",
    "            if Q[(i,j,1,0)] < Q[(i,j,1,1)]:\n",
    "                print('hit',end=\"\\t\")\n",
    "            else:\n",
    "                print('stand',end=\"\\t\")\n",
    "        else:\n",
    "            print('?',end=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Bonus** : Compute by two different approximate way the average earning of a game using the policy computed with Q-learning. (The approximate one given by the Q learning algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Q,mean_reward \u001b[38;5;241m=\u001b[39m \u001b[43mqlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mqlearning\u001b[1;34m(epsilon, discount_factor, learning_rate, I, q0, debug_info)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#choose best action with probability 1-epsilon\u001b[39;00m\n\u001b[0;32m     23\u001b[0m astar \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (v[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m v[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 24\u001b[0m a \u001b[38;5;241m=\u001b[39m astar \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn() \u001b[38;5;241m>\u001b[39m \u001b[43mepsilon\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_info: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen action\u001b[39m\u001b[38;5;124m'\u001b[39m,a)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#observe next state\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "Q,mean_reward = qlearning(0.1,1,0.03,1000000,0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 200000 episodes: -0.0614\n"
     ]
    }
   ],
   "source": [
    "def get_episode(Q):\n",
    "    #STEP 1: START\n",
    "    episode_visited_states = []\n",
    "    #deal cards to both players \n",
    "    s = initialize_episode()\n",
    "    #STEP 2: PLAYERS TURN\n",
    "    #player plays according to a fixed policy until he has either gone bust or has chosen to stand\n",
    "    end_player = False\n",
    "    while not end_player:\n",
    "        a = Q[(s[0],s[1],s[2],0)] < Q[(s[0],s[1],s[2],1)]\n",
    "        (s,end_player) = next_state(s,a)\n",
    "    #STEP 3: DEALERS TURN\n",
    "    #dealer plays by hitting until its value is above 17 and the outcome of the game is decided there\n",
    "    if (a == 1): episode_visited_states.append((s[0],s[1],s[2]))\n",
    "    episode_reward = get_reward(s)\n",
    "    return (episode_reward,episode_visited_states)\n",
    "\n",
    "\n",
    "I = 200000\n",
    "r = np.zeros(I)\n",
    "for i in range(I):\n",
    "    r[i], _ = get_episode(Q)\n",
    "print(f'Average reward over {I} episodes: {np.mean(r):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
